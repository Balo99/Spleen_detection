{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_OnWZHoAszC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Neural Imaging\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "import torchio as tio\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchmetrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6yHnp-FMy78",
        "outputId": "fe2ea0bf-a6bc-4798-80ad-b6aa22f9d67b"
      },
      "outputs": [],
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
        "print(device)\n",
        "experiment_name = 'prova'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChJBcSN5NEWs"
      },
      "source": [
        "Loading the Data (original version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the original images that I downloaded from the website of the Decathlon Challenge (\"http://medicaldecathlon.com\"). \n",
        "In this section I upload the files, creating a list with the names of all the files and applied N4 Bias Field Correction to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRSIbdOPA1LL",
        "outputId": "d8099c50-8816-4a0f-c9dd-9ec45f7e5b58"
      },
      "outputs": [],
      "source": [
        "#data_path_tr=\"/content/drive/MyDrive/Task09_Spleen/imagesTr\"\n",
        "#data_path_ts=\"/content/drive/MyDrive/Task09_Spleen/imagesTs\"\n",
        "#labels_tr = \"/content/drive/MyDrive/Task09_Spleen/labelsTr\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY3XuaxbpKJs",
        "outputId": "135ec71d-3bfc-4241-dc86-f1f71b8cd661"
      },
      "outputs": [],
      "source": [
        "# List of path of all the training files\n",
        "\n",
        "#tr_names = glob.glob(data_path_tr + '/*')\n",
        "#ts_names = glob.glob(data_path_ts + '/*')\n",
        "#tr_labels = glob.glob(labels_tr + '/*')\n",
        "\n",
        "#Total Number of Training samples\n",
        "#len(tr_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsHl2A6uDUFd",
        "outputId": "ffe761e8-1e0b-449e-c9d0-872357007fd9"
      },
      "outputs": [],
      "source": [
        "#Get more informations from the Nifti format\n",
        "#lab = nib.load(tr_names + '/spleen_63.nii.gz').get_fdata()\n",
        "\n",
        "#Labels are already One Hot encoded into (0,1) format\n",
        "#np.unique(lab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXpbIJ_LX2wr"
      },
      "source": [
        "Applying N4 Bias Field Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxLC-i26I0KO"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact\n",
        "def explore_3D_array(arr: np.ndarray, cmap: str = 'gray'):\n",
        "\n",
        "  def fn(SLICE):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.imshow(arr[SLICE, :, :], cmap=cmap)\n",
        "\n",
        "  interact(fn, SLICE=(0, arr.shape[0]-1))\n",
        "\n",
        "#Function to cancel out the bias\n",
        "def bias_cancellation(imm_path: str):\n",
        "    \"\"\"\n",
        "    Correct the given image reducing the N4 Bias .\n",
        "\n",
        "    Args:\n",
        "        imm_path (str): string containing image path\n",
        "\n",
        "    Returns:\n",
        "        corrected_image_full_resolution (Image): 3D Image of all the slices after bias correction\n",
        "    \"\"\"\n",
        "    raw_img_sitk = sitk.ReadImage(imm_path, sitk.sitkFloat32)\n",
        "    raw_img_sitk_arr = sitk.GetArrayFromImage(raw_img_sitk)\n",
        "\n",
        "    #Creating the mask rescaling the intensity\n",
        "    transformed = sitk.RescaleIntensity(raw_img_sitk, 0, 255)\n",
        "    transformed = sitk.LiThreshold(transformed,0,1)\n",
        "    head_mask = transformed\n",
        "\n",
        "    #Applying the correction on a low resolution image for computational reasons\n",
        "    shrinkFactor = 4\n",
        "    inputImage = raw_img_sitk\n",
        "\n",
        "    inputImage = sitk.Shrink( raw_img_sitk, [ shrinkFactor ] * inputImage.GetDimension() )\n",
        "    maskImage = sitk.Shrink( head_mask, [ shrinkFactor ] * inputImage.GetDimension() )\n",
        "\n",
        "    bias_corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
        "\n",
        "    corrected = bias_corrector.Execute(inputImage, maskImage)\n",
        "\n",
        "    #Returning to the previous resolution\n",
        "    log_bias_field = bias_corrector.GetLogBiasFieldAsImage(raw_img_sitk)\n",
        "    corrected_image_full_resolution = raw_img_sitk / sitk.Exp( log_bias_field )\n",
        "\n",
        "    return corrected_image_full_resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYS0EqiEXfmF"
      },
      "outputs": [],
      "source": [
        "#This is used to create a new folder containing all the images saved after having applied bias correction\n",
        "#First I apply the function created before and then I will save all these images\n",
        "\n",
        "#for i in tr_names:\n",
        "#    corrected_image_full_resolution = bias_cancellation(i)\n",
        "#    new_name = i.replace('imagesTr', 'imagesTr_nobias')\n",
        "#    sitk.WriteImage(corrected_image_full_resolution, new_name)\n",
        "\n",
        "#for i in ts_names:\n",
        "#    corrected_image_full_resolution = bias_cancellation(i)\n",
        "#    new_name = i.replace('imagesTs', 'imagesTs_nobias')\n",
        "#    sitk.WriteImage(corrected_image_full_resolution, new_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the images after Bias Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#gdown for the data after bias correction\n",
        "path = 'dati_nobias.zip'\n",
        "gdown.download(id='1HMUZhx1tT5jYT5_YPpw_vJ8HBoYLYSg-', output= path, quiet = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Unpacking the zip file\n",
        "shutil.unpack_archive(\"dati_nobias.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfk-Qb9BmoAT"
      },
      "outputs": [],
      "source": [
        "#Creating the paths with the ones that have no bias\n",
        "tr_nobias= \"imagesTr_nobias\"\n",
        "labels_tr = \"labelsTr\"\n",
        "\n",
        "# List of path of all the training files\n",
        "tr_names_no = glob.glob(tr_nobias + '/*')\n",
        "tr_labels = glob.glob(labels_tr + '/*')\n",
        "\n",
        "tr_names_no = np.sort([path.replace(\"\\\\\", \"/\") for path in tr_names_no])\n",
        "tr_labels = np.sort([path.replace(\"\\\\\", \"/\") for path in tr_labels])\n",
        "\n",
        "print(len(tr_names_no))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0WG6NwDXYrJ"
      },
      "source": [
        "Visualizing some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFmC3egnCaoI",
        "outputId": "360b850a-cce6-4edf-bc60-ad6d722bc2cf"
      },
      "outputs": [],
      "source": [
        "img_1 = nib.load(tr_nobias + \"/spleen_2.nii.gz\").get_fdata()\n",
        "lab_1 = nib.load(labels_tr + \"/spleen_2.nii.gz\").get_fdata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "74xQNbTUCkX2",
        "outputId": "a8807a58-5031-4192-e37c-e4f1dc92e9e0"
      },
      "outputs": [],
      "source": [
        "#Selecting one slice to be shown\n",
        "z = 70\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 6))\n",
        "\n",
        "ax1.imshow(img_1[:,:,z], cmap = \"gray\")\n",
        "ax2.imshow(lab_1[:,:,z], cmap = \"rainbow\")\n",
        "ax3.imshow(img_1[:,:,z], cmap = \"gray\")\n",
        "ax3.imshow(lab_1[:,:,z], cmap = \"rainbow\", alpha = 0.5)\n",
        "\n",
        "\n",
        "ax1.set_title(\"MRI\")\n",
        "ax2.set_title(\"MASK\")\n",
        "ax3.set_title(\"MRI w/ MASK\")\n",
        "\n",
        "ax1.set_axis_off()\n",
        "ax2.set_axis_off()\n",
        "ax3.set_axis_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "xm6ccC_kSIvN",
        "outputId": "54a6b6cd-5279-47cf-b407-c4313cb08d50"
      },
      "outputs": [],
      "source": [
        "#Showing all the different slices in sequence\n",
        "fig, axes = plt.subplots(6, int(img_1.shape[2]/6), figsize=(16, 9))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(img_1[:,:,i], cmap=\"gray\")\n",
        "    ax.imshow(lab_1[:,:,i], cmap=\"rainbow\", alpha = 0.5)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKB_CoDDYasF",
        "outputId": "5d039854-891c-4069-d6c7-2877604af883"
      },
      "outputs": [],
      "source": [
        "#Searching for the smallest number of slides\n",
        "y = []\n",
        "for i in range(len(tr_names_no)):\n",
        "  image = nib.load(tr_names_no[i]).get_fdata()\n",
        "  y.append(image.shape[2])\n",
        "print(min(y))\n",
        "print(max(y))\n",
        "#Minimum is 31 slides\n",
        "#Maximum is 168 slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J5YRaiutaKh"
      },
      "source": [
        "## 2-D Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving a .npy copy of all the files from the .nii.gz formato for computational reasons.\n",
        "In this section I will transform all the .nii.gz in numpy array, one array per slice per patient.\n",
        "Loading for the training the nii.gz was computationally expensive, this is a faster way of approaching the problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" #Labels\n",
        "for i in tqdm(range(len(tr_labels))):\n",
        "  image = nib.load(tr_labels[i]).get_fdata()\n",
        "  z = image.shape[2]\n",
        "\n",
        "  for j in range(z):\n",
        "    np.save(\"C:/Users/user/Desktop/Artificial Intelligence/Piccoli/Labels_npy/\" +tr_labels[i].split(\".\")[0].split(\"/\")[-1] + \"_\" + str(j) + str(\".npy\"), image[:,:,j]) \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" #Images\n",
        "for i in tqdm(range(len(tr_names_no))):\n",
        "  image = nib.load(tr_names_no[i]).get_fdata()\n",
        "  z = image.shape[2]\n",
        "\n",
        "  for j in range(z):\n",
        "    np.save(\"C:/Users/user/Desktop/Artificial Intelligence/Piccoli/Images_npy/\" +tr_names_no[i].split(\".\")[0].split(\"/\")[-1] + \"_\" + str(j) + str(\".npy\"), image[:,:,j]) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section I will provide the gdown with the numpy file already created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#gdown for the data in npy format\n",
        "path = 'dati_numpy.zip'\n",
        "gdown.download(id='1TPxra-VksERcClu4jIO2SKD7i2B5Ct7P', output= path, quiet = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Unpacking the zip file\n",
        "shutil.unpack_archive(\"dati_numpy.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fVLFVJ-zdPG"
      },
      "source": [
        "To obtain a 2D implementation first I need to create a single DataFrame that contains all the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "6vo3QJK3zriz",
        "outputId": "634dcb86-508c-4d38-9157-75c0ccf3f12c"
      },
      "outputs": [],
      "source": [
        "#Creating a Dataframe with the path of all the nii.gz files\n",
        "lista_dati = pd.DataFrame(columns = [\"Path\", \"Slice\", \"Label Path\"])\n",
        "for i in tqdm(range(len(tr_names_no))):\n",
        "    image = nib.load(tr_names_no[i]).get_fdata()\n",
        "    z = image.shape[2]\n",
        "\n",
        "    df_slices = pd.DataFrame({\n",
        "        \"Path\": [tr_names_no[i]] * z,\n",
        "        \"Slice\": np.arange(0, z),\n",
        "        \"Label Path\": [tr_labels[i]] * z\n",
        "    })\n",
        "\n",
        "    # Append the slice DataFrame to the main dataset\n",
        "    lista_dati = pd.concat([lista_dati, df_slices], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Refreshing the paths with the ones that have no bias\n",
        "immagini_npy = \"Images_npy\"\n",
        "labels_npy = \"Labels_npy\"\n",
        "\n",
        "# List of path of all the training files\n",
        "names_npy_im = glob.glob(immagini_npy + '/*')\n",
        "names_npy_lb = glob.glob(labels_npy + '/*')\n",
        "\n",
        "names_npy_im = np.sort([path.replace(\"\\\\\", \"/\") for path in names_npy_im])\n",
        "names_npy_lb = np.sort([path.replace(\"\\\\\", \"/\") for path in names_npy_lb])\n",
        "\n",
        "#Creating a Dataframe with the path of all the npy files sorted\n",
        "lista_dati_2 = pd.DataFrame(columns = [\"Path\", \"Label Path\"])\n",
        "lista_dati_2['Path'] = names_npy_im\n",
        "lista_dati_2['Label Path'] = names_npy_lb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE1ARUvOKxZj"
      },
      "outputs": [],
      "source": [
        "# create a Pytorch Dataset class\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, type = \"npy\", usage = 'train'):\n",
        "      \"\"\"\n",
        "      Initializes a custom PyTorch dataset.\n",
        "\n",
        "      Args:\n",
        "          df (pandas.DataFrame): DataFrame containing image paths, labels path, slice\n",
        "          usage (str, optional): Usage mode ('train', 'test'). Defaults to 'train'.\n",
        "          type (str, optional): Input files type (\"npy\", \"nii.gz\"). Defaults to \"npy\".\n",
        "      \"\"\"\n",
        "\n",
        "      self.df = df\n",
        "      self.usage = usage\n",
        "      self.type = type\n",
        "\n",
        "      #Preprocessing:\n",
        "      #Normalization using ZNormalization\n",
        "      #Clamp - Using Hounsfield scale to improve visualization of the spleen\n",
        "      #RescaleIntensity\n",
        "      #Resampling to 1 mm isotropic for faster computation\n",
        "      HOUNSFIELD_AIR = -1000\n",
        "      HOUNSFIELD_BONE = 1400\n",
        "      self.transform = tio.Compose([\n",
        "          tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "          tio.Clamp(out_min=HOUNSFIELD_AIR, out_max=HOUNSFIELD_BONE),\n",
        "          tio.RescaleIntensity(out_min_max=(0, 1)),\n",
        "          tio.Resample(1),\n",
        "      ])\n",
        "\n",
        "      #Data Augmentation with standard MRI artifacts like:\n",
        "      #Bias Field\n",
        "      #Blurring\n",
        "      #Noise\n",
        "      #Spike\n",
        "      self.augment = tio.Compose([\n",
        "          tio.RandomBiasField(p=0.3),\n",
        "          tio.OneOf({\n",
        "              tio.RandomBlur(): 0.2,\n",
        "              tio.RandomNoise(mean = 0, std = 0.1) : 0.6,\n",
        "              tio.RandomSpike(): 0.2\n",
        "          })\n",
        "      ])\n",
        "\n",
        "      #Data Augmentation with random rotation\n",
        "      self.torch_augment = A.Compose([\n",
        "          A.augmentations.geometric.rotate.RandomRotate90(p = 0.5),\n",
        "      ])\n",
        "\n",
        "      #Resizing to a common shape of 224x224\n",
        "      self.resizing = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((224, 224), antialias=False),\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      # get filename and label\n",
        "      if self.type == \"nii\":\n",
        "        path = self.df.iloc[index]['Path']\n",
        "        sl = self.df.iloc[index]['Slice']\n",
        "        label_path = self.df.iloc[index]['Label Path']\n",
        "\n",
        "        # Load image\n",
        "        image = nib.load(path).get_fdata()[:,:,sl]\n",
        "\n",
        "        # Load label\n",
        "        label = nib.load(label_path).get_fdata()[:,:,sl]\n",
        "      \n",
        "      if self.type == \"npy\":\n",
        "        path = self.df.iloc[index]['Path']\n",
        "        label_path = self.df.iloc[index]['Label Path']\n",
        "\n",
        "        # Load image\n",
        "        image = np.load(path)\n",
        "\n",
        "        # Load label\n",
        "        label = np.load(label_path)\n",
        "\n",
        "      image = np.expand_dims(image, [0,3])\n",
        "      image = self.transform(image)\n",
        "\n",
        "      #Data augmentation only during train\n",
        "      if self.usage == 'train':\n",
        "        \n",
        "        #Data Augmentation with MRI Artifacts is applied only to the image, leaving the mask untouched\n",
        "        image = self.augment(image)\n",
        "        image = np.squeeze(image)\n",
        "\n",
        "        #Creating a dictionary that contains both image and mask\n",
        "        sample = {'image': image, 'mask': label}\n",
        "        #Data augmentation with random rotation to both image and mask\n",
        "        sample = self.torch_augment(image = sample['image'], mask = sample['mask'])\n",
        "\n",
        "        image = sample['image']\n",
        "        label = sample['mask']\n",
        "\n",
        "        #Resizing both mask and image\n",
        "        image = np.array(image)\n",
        "        image = self.resizing(image)\n",
        "        image = torch.squeeze(image)\n",
        "\n",
        "        label = np.array(label)\n",
        "        label = self.resizing(label)\n",
        "        label = torch.squeeze(label)\n",
        "\n",
        "      #When the Dataset is in test mode then don't apply data augmentation but only resizing\n",
        "      if self.usage == 'test':\n",
        "       \n",
        "        image = np.squeeze(image)\n",
        "        image = np.array(image)\n",
        "        image = self.resizing(image)\n",
        "        image = torch.squeeze(image)\n",
        "\n",
        "        label = np.array(label)\n",
        "        label = self.resizing(label)\n",
        "        label = torch.squeeze(label)\n",
        "      \n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "fdSfrAupOah4",
        "outputId": "462c0b5a-3551-4eca-fc79-525a1a80c060"
      },
      "outputs": [],
      "source": [
        "ds = Dataset(lista_dati_2, usage = \"train\")\n",
        "\n",
        "#Visualizing an example of MRI image with data augmentation\n",
        "featurs, label = ds.__getitem__(58)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 6))\n",
        "\n",
        "ax1.imshow(featurs, cmap = \"gray\")\n",
        "ax2.imshow(label, cmap = \"rainbow\")\n",
        "ax3.imshow(featurs, cmap = \"gray\")\n",
        "ax3.imshow(label, cmap = \"rainbow\", alpha = 0.5)\n",
        "\n",
        "ax1.set_title(\"MRI\")\n",
        "ax2.set_title(\"MASK\")\n",
        "ax3.set_title(\"MRI w/ MASK\")\n",
        "\n",
        "ax1.set_axis_off()\n",
        "ax2.set_axis_off()\n",
        "ax3.set_axis_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MmJXIdhErdh"
      },
      "outputs": [],
      "source": [
        "#Creating a Validation Set and a Test Set\n",
        "path_train, path_val = train_test_split(lista_dati_2, test_size = 0.3, random_state = 42)\n",
        "path_test, path_val = train_test_split(path_val, test_size = 0.5, random_state = 42)\n",
        "\n",
        "batch_size = 64\n",
        "# Create Dataset\n",
        "train_ds = Dataset(path_train)\n",
        "test_ds = Dataset(path_test, usage = 'test')\n",
        "val_ds = Dataset(path_val, usage = 'test')\n",
        "\n",
        "\n",
        "# Create Dataloader using different batch size for train and validation\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle = True, drop_last=True)\n",
        "valid_loader = DataLoader(val_ds, batch_size=16, shuffle = True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "#Emptying cuda cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHURpcqAMP_K"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "#Two different pretrained models:\n",
        "model = smp.DeepLabV3Plus(\n",
        "  encoder_name=\"resnet34\", \n",
        "  encoder_weights= \"imagenet\", \n",
        "  in_channels=1,  #model input channels \n",
        "  classes=2, # model output channels \n",
        ")\n",
        "\n",
        "#model = smp.Unet(\n",
        " #   encoder_name=\"resnet34\",        # choose encoder\n",
        " #   encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights \n",
        "  #  in_channels=1,                  # model input channels \n",
        "   # classes=2                       # model output channels (number of classes in your dataset)\n",
        "#)\n",
        "\n",
        "#Loading the model on the device\n",
        "model.to(device)\n",
        "\n",
        "#Choosing the optimizer, the criterion and the scheduler\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = len(train_loader)*5, # Maximum number of iterations.\n",
        "                             eta_min = 1e-5) # Minimum learning rate.\n",
        "\n",
        "#Initializing the variables to empty array\n",
        "train_losses = []\n",
        "val_ious = []\n",
        "val_losses = []\n",
        "\n",
        "for idx_epoch in range(NUM_EPOCHS):\n",
        "  print('train')\n",
        "  running_loss = 0.\n",
        "  #Training\n",
        "  model.train()\n",
        "  for image, mask in tqdm(train_loader):\n",
        "    # move to device\n",
        "    image = image.to(device)\n",
        "    mask = mask.to(device).squeeze()\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass\n",
        "    image = torch.unsqueeze(image, 1)\n",
        "    pred_mask = model(image.float())\n",
        "    # compute loss\n",
        "    loss = criterion(pred_mask, mask.long())\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    # update running loss\n",
        "    running_loss += loss.item()\n",
        "  train_losses.append(running_loss / len(train_loader))\n",
        "  print(f\"{idx_epoch:02d}: {train_losses[-1]:.6f}\")\n",
        "  print('validate')\n",
        "  ious, nan_i = 0., 0\n",
        "  running_loss = 0.\n",
        "  #Validation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for image, mask in valid_loader:\n",
        "      # move to device\n",
        "      image = image.to(device)\n",
        "      image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "      mask = mask.to(device).long()\n",
        "      mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "      pred_mask = model(image.float())\n",
        "\n",
        "      #select the probabilities for the foreground class and create a mask accordingly\n",
        "      pred_msk_binary = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "      image = torch.squeeze(image)\n",
        "      mask = torch.squeeze(mask)\n",
        "      #Using the IoU Metric during the validation\n",
        "      metric = torchmetrics.JaccardIndex(task = \"binary\").to(device)\n",
        "      iou = metric(pred_msk_binary, mask)\n",
        "      #If IoU is nan then increasing by 1 the total number of nan found\n",
        "      if torch.isnan(iou):\n",
        "        nan_i += 1\n",
        "      ious = torch.nansum(torch.tensor([ious,iou]))\n",
        "      loss = criterion(pred_mask, mask)\n",
        "      running_loss += loss.item()\n",
        "  val_ious.append(ious.cpu()/(len(valid_loader)-len(nan_i)))\n",
        "  val_losses.append(running_loss / len(valid_loader))\n",
        "  print(f\"iou: {val_ious[-1]:.4f}\")\n",
        "\n",
        "  if np.argmax(val_ious) == len(val_ious)-1:\n",
        "    print('new best model')\n",
        "    torch.save(model.state_dict(), 'deeplab.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the code using the Test Dataset and two different metrics: Dice + IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loading the pretrained model using gdown\n",
        "\n",
        "def load_model_2D(path = None, which = None):\n",
        "#Download the two different models depending on the choice\n",
        "    if path is None:\n",
        "        print(\"We are using Unet\")\n",
        "        id = '1B5pIMRFd0FywUdexZ5Ah6eoiI_9CfHag'\n",
        "        path = 'model_unet2d.pth'\n",
        "        gdown.download(id=id, output=path, quiet=False)\n",
        "    \n",
        "    if which == \"unet\": \n",
        "        model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "        in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "        classes=2\n",
        "        )\n",
        "\n",
        "        gdown.download(id='1B5pIMRFd0FywUdexZ5Ah6eoiI_9CfHag', output=path, quiet=True)\n",
        "\n",
        "    if which == \"deeplab\":\n",
        "        model = smp.DeepLabV3Plus(\n",
        "        encoder_name=\"resnet34\", \n",
        "        encoder_weights= \"imagenet\", \n",
        "        in_channels=1,  #model input channels \n",
        "        classes=2, # model output channels \n",
        "        )\n",
        "\n",
        "        gdown.download(id='1XtWzG3hdKUHDGQ1OnoGlPPG3LJBIDAbx', output=path, quiet=True)\n",
        "        \n",
        "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model_2D(\"unet_prova.pth\", \"unet\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Test Dataloader using a batchsize of 1\n",
        "\n",
        "dl_test = DataLoader(test_ds, shuffle=False, batch_size=1, num_workers=0)\n",
        "i = 0\n",
        "\n",
        "for image, label in dl_test:\n",
        "  image = image.to(device)\n",
        "  image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "  mask = label.to(device).long()\n",
        "  mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "  pred_mask = model(image.float())\n",
        "\n",
        "  predicted = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "  image = torch.squeeze(image)\n",
        "  mask = torch.squeeze(mask)\n",
        "  #Plotting the first ten images to show how the model worked in a qualitatively way\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 6))\n",
        "\n",
        "  ax1.imshow(image.cpu(), cmap = \"gray\")\n",
        "  ax2.imshow(image.cpu(), cmap = \"gray\")\n",
        "  ax2.imshow(mask.cpu(), cmap = \"rainbow\", alpha = 0.5)\n",
        "  ax3.imshow(image.cpu(), cmap = \"gray\")\n",
        "  ax3.imshow(predicted.cpu()[0], cmap = \"rainbow\", alpha = 0.5)\n",
        "\n",
        "  ax1.set_title(\"MRI\")\n",
        "  ax2.set_title(\"TRUE MASK\")\n",
        "  ax3.set_title(\"PREDICTED MASK\")\n",
        "\n",
        "  ax1.set_axis_off()\n",
        "  ax2.set_axis_off()\n",
        "  ax3.set_axis_off()\n",
        "\n",
        "  if i == 10:\n",
        "    break\n",
        "  \n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchmetrics.classification import Dice\n",
        "\n",
        "ious, dices, nan_i =0, 0, 0\n",
        "val_ious, val_dices = [],[]\n",
        "\n",
        "for image, label in dl_test:\n",
        "  image = image.to(device)\n",
        "  image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "  mask = label.to(device).long()\n",
        "  mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "  pred_mask = model(image.float())\n",
        "\n",
        "  predicted = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "  image = torch.squeeze(image)\n",
        "  mask = torch.squeeze(mask)\n",
        "  predicted = torch.squeeze(predicted)\n",
        "  \n",
        "  #Measuring the IoU metric using the Jaccard Index\n",
        "  metric = torchmetrics.JaccardIndex(task = \"binary\").to(device)\n",
        "  iou = metric(predicted, mask)\n",
        "  if torch.isnan(iou):\n",
        "    nan_i += 1\n",
        "  ious = torch.nansum(torch.tensor([ious,iou]))\n",
        "  metric_bis = torchmetrics.Dice(num_classes=2, average='macro').to(device)\n",
        "  dice = metric_bis(predicted, mask)\n",
        "  dices = torch.sum(torch.tensor([dices,dice]))\n",
        "\n",
        "#Measuring the average iou and dice on all the dataset\n",
        "val_ious.append(ious.cpu()/(len(dl_test)-nan_i))\n",
        "val_dices.append(dices.cpu()/len(dl_test))\n",
        "print(f\"Test IoU: {val_ious[-1]:.4f}\")\n",
        "print(f\"Test Dice: {val_dices[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keXjnwnZfKzn"
      },
      "source": [
        "## 3-D Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First I will create a dataframe for the 3D implementation with the image path, the path of the corresponding label and the index of the first slice in which I have a mask different from zero and the number of slices in which the mask is not all zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JaKhiCmMbLE",
        "outputId": "132867a6-1d65-449f-816d-d93c4102e740"
      },
      "outputs": [],
      "source": [
        "slices_3d = pd.DataFrame(columns = [\"Path\", \"Label Path\", \"Start Index\", \"Num_slices\"])\n",
        "\n",
        "for i in tqdm(range(len(tr_names_no))):\n",
        "\n",
        "  image = nib.load(tr_names_no[i]).get_fdata()\n",
        "  mask = nib.load(tr_labels[i]).get_fdata()\n",
        "\n",
        "  mask_start_idx = 0\n",
        "  while mask[:,:,mask_start_idx].max() == 0:\n",
        "    mask_start_idx += 1\n",
        "  mask_end_idx = mask_start_idx\n",
        "  while mask[:,:,mask_end_idx].max() != 0:\n",
        "    mask_end_idx += 1\n",
        "\n",
        "  num_spleen_slices = mask_end_idx - mask_start_idx\n",
        "\n",
        "  df_slices = pd.DataFrame({\n",
        "      \"Path\": [tr_names_no[i]],\n",
        "      \"Label Path\": [tr_labels[i]],\n",
        "      \"Start Index\": mask_start_idx,\n",
        "      \"Num_slices\": num_spleen_slices\n",
        "  })\n",
        "\n",
        "  slices_3d = pd.concat([slices_3d, df_slices], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stack_images_in_blocks(df, idx, block_size=32):\n",
        "    \"\"\"Stacks images and labels from a DataFrame into blocks of given size.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame containing image paths and label paths.\n",
        "        idx (int or list): Index or list of indices specifying which images to stack.\n",
        "        block_size (int, optional): Size of each block. Defaults to 32.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two NumPy arrays:\n",
        "            stacked_images (np.ndarray): Array of stacked images with shape (num_blocks, height, width, block_size).\n",
        "            stacked_labels (np.ndarray): Array of stacked labels with the same shape as stacked_images.\"\"\"\n",
        "\n",
        "    img = nib.load(df[\"Path\"][idx])\n",
        "    lab = nib.load(df[\"Label Path\"][idx])\n",
        "        \n",
        "    data = img.get_fdata()\n",
        "    label = lab.get_fdata()\n",
        "\n",
        "    height, width, num_images = data.shape\n",
        "\n",
        "    # Check if block size is a multiple of variable dimension\n",
        "    remainder = num_images % block_size\n",
        "    if remainder != 0:\n",
        "        print(f\"Warning: Discarding {remainder} columns from each image as they don't fit complete blocks.\")\n",
        "\n",
        "    num_blocks = num_images // block_size\n",
        "\n",
        "    # Initialize empty array to store stacked images\n",
        "    stacked_images = np.zeros((num_blocks, height, width, block_size))\n",
        "    stacked_labels = np.zeros((num_blocks, height, width, block_size))\n",
        "\n",
        "    # Loop over images and stack them block-wise\n",
        "    for block_index in range(num_blocks):\n",
        "        start_index = block_index * block_size\n",
        "        end_index = min(start_index + block_size, num_images)\n",
        "        stacked_images[block_index, :, :, :end_index - start_index] = data[:, :, start_index:end_index]\n",
        "        stacked_labels[block_index, :, :, :end_index - start_index] = label[:, :, start_index:end_index]\n",
        "        \n",
        "    return stacked_images, stacked_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating block of stacked images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, _ in slices_3d.iterrows():\n",
        "    stacked_images, stacked_labels = stack_images_in_blocks(slices_3d, index)\n",
        "    for j in range(stacked_images.shape[0]):\n",
        "        np.save(slices_3d[\"Path\"][index].replace(\"imagesTr_nobias\", \"images_npy_3d\").replace(\".nii.gz\", \"_\" + str(j) + \".npy\"), stacked_images[j,:,:,:])\n",
        "        np.save(slices_3d[\"Label Path\"][index].replace(\"labelsTr\", \"labels_npy_3d\").replace(\".nii.gz\", \"_\" + str(j) + \".npy\"), stacked_labels[j,:,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For simplicity I will upload the files already in .npy format ready to be downloaded here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"dati_numpy_3d.zip\"\n",
        "gdown(id = \"17my4ppZsjlAxlzTEVNUHpgcqjL6ffREI\", output = path, quiet = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Unpacking the zip file\n",
        "shutil.unpack_archive(\"dati_numpy_3d.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "immagini_npy_3d = \"images_npy_3d\"\n",
        "labels_npy_3d = \"labels_npy_3d\"\n",
        "\n",
        "# List of path of all the training files\n",
        "names_npy_im_3d = glob.glob(immagini_npy_3d + '/*')\n",
        "names_npy_lb_3d = glob.glob(labels_npy_3d + '/*')\n",
        "\n",
        "names_npy_im_3d = np.sort([path.replace(\"\\\\\", \"/\") for path in names_npy_im_3d])\n",
        "names_npy_lb_3d = np.sort([path.replace(\"\\\\\", \"/\") for path in names_npy_lb_3d])\n",
        "\n",
        "#Creating a Dataframe with the path of all the npy files sorted\n",
        "lista_dati_3 = pd.DataFrame(columns = [\"Path\", \"Label Path\"])\n",
        "lista_dati_3['Path'] = names_npy_im_3d\n",
        "lista_dati_3['Label Path'] = names_npy_lb_3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DSYbdjpUXniv",
        "outputId": "03959143-85fd-4b0a-cceb-c08d217df10f"
      },
      "outputs": [],
      "source": [
        "# create a Pytorch Dataset class\n",
        "class Dataset_3D(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, usage = 'train'):\n",
        "      \"\"\"\n",
        "      Initializes a custom PyTorch dataset.\n",
        "\n",
        "      Args:\n",
        "          df (pandas.DataFrame): DataFrame containing image paths, labels path, slice\n",
        "          usage (str, optional): Usage mode ('train', 'val', 'test'). Defaults to 'train'.\n",
        "      \"\"\"\n",
        "\n",
        "      self.df = df\n",
        "      self.usage = usage\n",
        "\n",
        "      #Preprocessing:\n",
        "      #Clamp - Using Hounsfield scale to improve visualization of the spleen\n",
        "      #RescaleIntensity\n",
        "      #Resampling to 1 mm isotropic for faster computation\n",
        "      HOUNSFIELD_AIR = -1000\n",
        "      HOUNSFIELD_BONE = 1400\n",
        "      self.transform = tio.Compose([\n",
        "          tio.ToCanonical(),\n",
        "          tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
        "          tio.Clamp(out_min=HOUNSFIELD_AIR, out_max=HOUNSFIELD_BONE),\n",
        "          tio.RescaleIntensity(out_min_max=(0, 1)),\n",
        "          tio.Resample(1)\n",
        "      ])\n",
        "\n",
        "      self.augment = tio.Compose([\n",
        "          tio.RandomBiasField(p=0.3),\n",
        "          tio.OneOf({\n",
        "              tio.RandomBlur(): 0.2,\n",
        "              tio.RandomNoise(mean = 0, std = 0.05) : 0.6,\n",
        "              tio.RandomSpike(): 0.2\n",
        "          })\n",
        "      ])\n",
        "\n",
        "      self.resizing = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((224, 224), antialias=False),\n",
        "    ])\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      # get filename and label\n",
        "      path = self.df.iloc[index]['Path']\n",
        "      l_path = self.df.iloc[index]['Label Path']\n",
        "\n",
        "      # Load image\n",
        "      try:\n",
        "        im = np.load(path)\n",
        "        ms = np.load(l_path)\n",
        "        #For the files with dimension smaller than 32 (the minimum is 31) I add a slice of all zeros\n",
        "        if im.shape[2] == 31:\n",
        "           im = np.concatenate((im, np.zeros((512, 512, 1))), axis=-1)\n",
        "           ms = np.concatenate((ms, np.zeros((512, 512, 1))), axis=-1)\n",
        "        #Apply resizing\n",
        "        im = self.resizing(im)\n",
        "        ms = self.resizing(ms)\n",
        "\n",
        "        im = torch.unsqueeze(im, dim = 0)\n",
        "        im = self.transform(im)\n",
        "      except:\n",
        "        print(path, l_path, index)\n",
        "        return np.zeros((2, 224, 224))\n",
        "\n",
        "      #Data augmentation only during train\n",
        "      #Will use artificial artifacts to which MRI is often susceptible\n",
        "      if self.usage == 'train':\n",
        "        image = self.augment(im)\n",
        "        ms = torch.unsqueeze(ms, dim = 0)\n",
        "\n",
        "        image = np.array(image)\n",
        "        ms = np.array(ms)\n",
        "\n",
        "        image = np.squeeze(image)\n",
        "        ms = np.squeeze(ms)\n",
        "      if self.usage == 'test':\n",
        "        image = np.squeeze(im)\n",
        "\n",
        "\n",
        "      return image, ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bq-SBRakkzu"
      },
      "outputs": [],
      "source": [
        "ds = Dataset_3D(lista_dati_3, usage = \"train\")\n",
        "\n",
        "featurs, label = ds.__getitem__(16)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 6))\n",
        "\n",
        "ax1.imshow(featurs[15,:,:], cmap = \"gray\")\n",
        "ax2.imshow(label[15,:,:], cmap = \"rainbow\")\n",
        "ax3.imshow(featurs[15,:,:], cmap = \"gray\")\n",
        "ax3.imshow(label[15,:,:], cmap = \"rainbow\", alpha = 0.5)\n",
        "\n",
        "ax1.set_title(\"MRI\")\n",
        "ax2.set_title(\"MASK\")\n",
        "ax3.set_title(\"MRI w/ MASK\")\n",
        "\n",
        "ax1.set_axis_off()\n",
        "ax2.set_axis_off()\n",
        "ax3.set_axis_off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFsbw5rZnjaw"
      },
      "outputs": [],
      "source": [
        "#Creating a Validation Set and a Test Set\n",
        "path_train, path_val = train_test_split(lista_dati_3, test_size = 0.3, random_state = 42)\n",
        "path_test, path_val = train_test_split(path_val, test_size = 0.5, random_state = 42)\n",
        "\n",
        "batch_size = 6\n",
        "# Create Dataset\n",
        "train_ds = Dataset_3D(path_train)\n",
        "test_ds = Dataset_3D(path_test, usage = 'test')\n",
        "val_ds = Dataset_3D(path_val, usage = 'test')\n",
        "\n",
        "\n",
        "# Create Dataloader\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle = True, drop_last=True)\n",
        "valid_loader = DataLoader(val_ds, batch_size=1, shuffle = True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import monai\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "\n",
        "#Using the UNet pretrained model from the Monai library\n",
        "model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    channels=(16, 32, 64, 128, 256),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        "    norm=Norm.BATCH,\n",
        ").to(device)\n",
        "#Defining loss, optimizer and metric\n",
        "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "\n",
        "#Loading the pretrained model\n",
        "pretrained_model = monai.bundle.load(\n",
        "    name=\"spleen_ct_segmentation\", bundle_dir=\"./\"\n",
        ")\n",
        "\n",
        "model.load_state_dict(pretrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_ious = []\n",
        "val_losses = []\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "for idx_epoch in range(NUM_EPOCHS):\n",
        "  #Training\n",
        "  print('train')\n",
        "  running_loss = 0.\n",
        "  model.train()\n",
        "  for image, mask in tqdm(train_loader):\n",
        "    # move to device\n",
        "    image = image.to(device)\n",
        "    mask = mask.to(device).squeeze()\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass\n",
        "    image = torch.unsqueeze(image, 1)\n",
        "    pred_mask = model(image.float())\n",
        "    # compute loss\n",
        "\n",
        "    mask = torch.unsqueeze(mask, dim = 1)\n",
        "    loss = loss_function(pred_mask, mask.long())\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "    # update running loss\n",
        "    running_loss += loss.item()\n",
        "  train_losses.append(running_loss / len(train_loader))\n",
        "  print(f\"{idx_epoch:02d}: {train_losses[-1]:.6f}\")\n",
        "  print('validate')\n",
        "  ious = 0.\n",
        "  nan_i = 0\n",
        "  running_loss = 0.\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for image, mask in valid_loader:\n",
        "      # move to device\n",
        "      image = image.to(device)\n",
        "      image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "      mask = mask.to(device).long()\n",
        "      mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "      pred_mask = model(image.float())\n",
        "\n",
        "      #select the probabilities for the foreground class and create a mask accordingly\n",
        "      pred_msk_binary = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "\n",
        "      image = torch.squeeze(image)\n",
        "      mask = torch.squeeze(mask, dim = 0)\n",
        "\n",
        "      metric = torchmetrics.JaccardIndex(task = \"binary\").to(device)\n",
        "      iou = metric(pred_msk_binary, mask)\n",
        "      if torch.isnan(iou):\n",
        "        nan_i += 1\n",
        "      ious = torch.nansum(torch.tensor([ious,iou]))\n",
        "      loss = criterion(pred_mask, mask)\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "  val_ious.append(ious.cpu()/(len(valid_loader)- nan_i))\n",
        "  val_losses.append(running_loss / len(valid_loader))\n",
        "  print(f\"iou: {val_ious[-1]:.4f}\")\n",
        "\n",
        "  if np.argmax(val_ious) == len(val_ious)-1:\n",
        "    print('new best model')\n",
        "    torch.save(model.state_dict(), 'unet3d_dice.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the pretrained 3D Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_3dmodel(path = None):\n",
        "    # If a copy of the model is present on the pc, load it from the path\n",
        "    # Download it otherwise\n",
        "    if path is None:\n",
        "        id = '1oh1_ngh9vvC-iO_-1d4v5lsX22ioW4cI'\n",
        "        path = '3dmodel_best.pth'\n",
        "        gdown.download(id=id, output=path, quiet=False)\n",
        "    \n",
        "    model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    channels=(16, 32, 64, 128, 256),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        "    norm=Norm.BATCH,\n",
        "    ).to(device)\n",
        "\n",
        "    state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_3dmodel(\"prova_3d.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the Model using both IoU and Dice Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#DataLoader\n",
        "dl_test = DataLoader(test_ds, shuffle=True, batch_size=1, num_workers=0)\n",
        "i, nan_i = 0, 0\n",
        "ious, dices, val_ious_test, val_dices = [], [], [], []\n",
        "\n",
        "for image, mask in dl_test:\n",
        "  image = image.to(device)\n",
        "  image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "  mask = mask.to(device).long()\n",
        "  mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "  pred_mask = model(image.float())\n",
        "\n",
        "  predicted = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "  image = torch.squeeze(image)\n",
        "  mask = torch.squeeze(mask)\n",
        "  predicted = torch.squeeze(predicted)\n",
        "\n",
        "  metric = torchmetrics.JaccardIndex(task = \"binary\").to(device)\n",
        "  \n",
        "  iou = metric(predicted, mask)\n",
        "  if torch.isnan(iou):\n",
        "    nan_i += 1\n",
        "  ious = torch.nansum(torch.tensor([ious,iou]))\n",
        "  metric_bis = torchmetrics.Dice(num_classes=2, average='macro').to(device)\n",
        "  dice = metric_bis(predicted, mask)\n",
        "  dices = torch.sum(torch.tensor([dices,dice]))\n",
        "\n",
        "val_ious_test.append(ious.cpu()/(len(dl_test)-nan_i))\n",
        "val_dices.append(dices.cpu()/len(dl_test))\n",
        "print(f\"Test IoU: {val_ious_test[-1]:.4f}\")\n",
        "print(f\"Test Dice: {val_dices[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for image, label in dl_test:\n",
        "  image = image.to(device)\n",
        "  image = torch.unsqueeze(image, dim = 1)\n",
        "\n",
        "  mask = label.to(device).long()\n",
        "  mask = torch.unsqueeze(mask, dim = 1)\n",
        "\n",
        "  pred_mask = model(image.float())\n",
        "\n",
        "  predicted = torch.where(F.softmax(pred_mask, dim=1)[:, 1] >= 0.5, 1, 0)\n",
        "  image = torch.squeeze(image)\n",
        "  mask = torch.squeeze(mask)\n",
        "  predicted = torch.squeeze(predicted)\n",
        "\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 6))\n",
        "\n",
        "  ax1.imshow(image.cpu()[25], cmap = \"gray\")\n",
        "  ax2.imshow(image.cpu()[25], cmap = \"gray\")\n",
        "  ax2.imshow(mask.cpu()[25], cmap = \"rainbow\", alpha = 0.5)\n",
        "  ax3.imshow(image.cpu()[25], cmap = \"gray\")\n",
        "  ax3.imshow(predicted.cpu()[25], cmap = \"rainbow\", alpha = 0.5)\n",
        "\n",
        "  ax1.set_title(\"MRI\")\n",
        "  ax2.set_title(\"TRUE MASK\")\n",
        "  ax3.set_title(\"PREDICTED MASK\")\n",
        "\n",
        "  ax1.set_axis_off()\n",
        "  ax2.set_axis_off()\n",
        "  ax3.set_axis_off()\n",
        "  \n",
        "  if i == 10:\n",
        "    break\n",
        "  \n",
        "  i += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
